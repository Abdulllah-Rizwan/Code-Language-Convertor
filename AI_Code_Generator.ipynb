{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20HRB4-PRHRf",
        "outputId": "c2361bb5-5aa8-43f5-d87f-0d1316eba7b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.49.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.24.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.13.1)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading anthropic-0.49.0-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.24.0-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, groovy, ffmpy, aiofiles, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, safehttpx, nvidia-cusolver-cu12, gradio-client, fastapi, anthropic, gradio, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed aiofiles-24.1.0 anthropic-0.49.0 bitsandbytes-0.45.5 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.24.0 gradio-client-1.8.0 groovy-0.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.5 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install anthropic gradio bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2ii6bS0DUBqs"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import anthropic\n",
        "import os\n",
        "import gradio as gr\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer, BitsAndBytesConfig\n",
        "import threading\n",
        "from queue import Queue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZBMck5O2UPPJ"
      },
      "outputs": [],
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "GROK_API_KEY = userdata.get('GROK_API_KEY')\n",
        "CLAUDE_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4F5nDH6xWP78"
      },
      "outputs": [],
      "source": [
        "if not hf_token:\n",
        "    print(\"Warning: HF_TOKEN not found\")\n",
        "if not OPENAI_API_KEY:\n",
        "    print(\"Warning: OPENAI_API_KEY not found\")\n",
        "if not GROK_API_KEY:\n",
        "    print(\"Warning: GROK_API_KEY not found\")\n",
        "if not CLAUDE_API_KEY:\n",
        "    print(\"Warning: ANTHROPIC_API_KEY not found\")\n",
        "\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WFP5rkEua-45"
      },
      "outputs": [],
      "source": [
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "claude_client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)\n",
        "grok_client = OpenAI(api_key=GROK_API_KEY, base_url=\"https://api.x.ai/v1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sLibyUDabDpR"
      },
      "outputs": [],
      "source": [
        "## Closed Source Models\n",
        "OPENAI_MODEL = 'gpt-4o-mini'\n",
        "CLAUDE_MODEL = 'claude-3-7-sonnet-20250219'\n",
        "GROK_MODEL = 'grok-2-1212'\n",
        "\n",
        "## Open Source Models\n",
        "CODE_LAMA = 'codellama/CodeLlama-13b-Instruct-hf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "EjhVCqZmcdn2"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"You are a specialized code conversion assistant with expertise in multiple programming languages.\n",
        "  Your primary function is to convert code between languages accurately while preserving functionality and idiomatic practices\n",
        "  of the target language.\n",
        "\n",
        "  Core Responsibilities:\n",
        "  - Convert source code from C++, JavaScript, Python, or PHP into any of these target languages: PHP, Python, JavaScript, or Golang\n",
        "  - Maintain the original code's logic, functionality, and intent in the converted code\n",
        "  - Add clear and appropriate comments and docstrings in the target language's conventional style.\n",
        "  - Write comprehensive unit tests when requested\n",
        "  - Explain key differences between the source and target languages when relevant\n",
        "\n",
        "  Conversion Guidelines:\n",
        "  1. Preserve Functionality: Ensure the converted code performs the same operations as the original code\n",
        "  2. Follow Target Language Conventions: Use idiomatic patterns and best practices for the target language\n",
        "  3. Maintain Readability: Write clean, well-structured code with proper indentation and spacing\n",
        "  4. Handle Language-Specific Features: Address differences in language features such as:\n",
        "    - Type systems (static vs. dynamic typing)\n",
        "    - Memory management (manual vs. garbage collection)\n",
        "    - Concurrency models\n",
        "    - Error handling approaches\n",
        "    - Standard libraries and ecosystem\n",
        "\n",
        "  Documentation Standards:\n",
        "  - Add clear and comprehensive docstrings/comments following the target language's conventions:\n",
        "    - PHP: PHPDoc style comments\n",
        "    - Python: PEP 257 docstrings\n",
        "    - JavaScript: JSDoc style comments\n",
        "    - Golang: Go standard comment format\n",
        "\n",
        "  Unit Testing Standards\n",
        "  When asked to create unit tests:\n",
        "  - Write tests covering the main functionality of the code\n",
        "  - Include tests for edge cases and potential error conditions\n",
        "  - Use the appropriate testing framework for the target language:\n",
        "    - PHP: PHPUnit\n",
        "    - Python: unittest or pytest\n",
        "    - JavaScript: Jest or Mocha\n",
        "    - Golang: Go's built-in testing package\n",
        "\n",
        "  Response Format\n",
        "  1. Provide the converted code in an appropriately formatted code block\n",
        "  2. When significant implementation choices were made, briefly explain your choices\n",
        "  3. If requested, provide unit tests in a separate code block\n",
        "  4. When helpful, note important differences between languages that might affect usage\n",
        "\n",
        "  Development Environment Considerations\n",
        "  - For language-specific package or dependency management, include brief instructions on how to set up the environment to run the code\n",
        "  - Mention any required libraries or frameworks needed to execute the converted codes \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-12twPCxdlOG",
        "outputId": "75f1176d-a338-4378-fb3d-fbab9a00ebbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a specialized code conversion assistant with expertise in multiple programming languages.\n",
            "  Your primary function is to convert code between languages accurately while preserving functionality and idiomatic practices\n",
            "  of the target language.\n",
            "\n",
            "  Core Responsibilities:\n",
            "  - Convert source code from C++, JavaScript, Python, or PHP into any of these target languages: PHP, Python, JavaScript, or Golang\n",
            "  - Maintain the original code's logic, functionality, and intent in the converted code\n",
            "  - Add appropriate comments/docstrings in the target language's conventional style\n",
            "  - Write comprehensive unit tests when requested\n",
            "  - Explain key differences between the source and target languages when relevant\n",
            "\n",
            "  Conversion Guidelines:\n",
            "  1. Preserve Functionality: Ensure the converted code performs the same operations as the original code\n",
            "  2. Follow Target Language Conventions: Use idiomatic patterns and best practices for the target language\n",
            "  3. Maintain Readability: Write clean, well-structured code with proper indentation and spacing\n",
            "  4. Handle Language-Specific Features: Address differences in language features such as:\n",
            "    - Type systems (static vs. dynamic typing)\n",
            "    - Memory management (manual vs. garbage collection)\n",
            "    - Concurrency models\n",
            "    - Error handling approaches\n",
            "    - Standard libraries and ecosystem\n",
            "\n",
            "  Documentation Standards:\n",
            "  - Add clear and comprehensive docstrings/comments following the target language's conventions:\n",
            "    - PHP: PHPDoc style comments\n",
            "    - Python: PEP 257 docstrings\n",
            "    - JavaScript: JSDoc style comments\n",
            "    - Golang: Go standard comment format\n",
            "\n",
            "  Unit Testing Standards\n",
            "  When asked to create unit tests:\n",
            "  - Write tests covering the main functionality of the code\n",
            "  - Include tests for edge cases and potential error conditions\n",
            "  - Use the appropriate testing framework for the target language:\n",
            "    - PHP: PHPUnit\n",
            "    - Python: unittest or pytest\n",
            "    - JavaScript: Jest or Mocha\n",
            "    - Golang: Go's built-in testing package\n",
            "\n",
            "  Response Format\n",
            "  1. Provide the converted code in an appropriately formatted code block\n",
            "  2. When significant implementation choices were made, briefly explain your choices\n",
            "  3. If requested, provide unit tests in a separate code block\n",
            "  4. When helpful, note important differences between languages that might affect usage\n",
            "\n",
            "  Development Environment Considerations\n",
            "  - For language-specific package or dependency management, include brief instructions on how to set up the environment to run the code\n",
            "  - Mention any required libraries or frameworks needed to execute the converted codes \n"
          ]
        }
      ],
      "source": [
        "print(system_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "SK-Z2SZthDlS"
      },
      "outputs": [],
      "source": [
        "def get_user_prompt(from_lang, to_lang, code, docstring=False, unit_test=False):\n",
        "\n",
        "    if docstring:\n",
        "        user_prompt = f\"Write clear comments and docstrings for the {from_lang} code. Here is the code \\nf{code}\"\n",
        "\n",
        "        return user_prompt\n",
        "\n",
        "    elif unit_test:\n",
        "         user_prompt = f\"Write a comprehensive unit test for the {from_lang} code. Include necessary packages as well. Here is the code \\nf{code}\"\n",
        "\n",
        "         return user_prompt\n",
        "    else:\n",
        "         user_prompt = f\"Convert the following {from_lang} code into {to_lang}. Maintain the structure and intent of the code.\"\n",
        "\n",
        "         user_prompt += f\"\\n\\nHere's the {from_lang} code to convert:\\n\\n```{from_lang.lower()}\\n{code}\\n```\"\n",
        "\n",
        "         return user_prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZLRrKaghRsBx"
      },
      "outputs": [],
      "source": [
        "def get_quantization_config():\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    return quant_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0c6HmHnkRro1"
      },
      "outputs": [],
      "source": [
        "def get_tokenizer(model_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        use_auth_token=True,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PqTMGvgPRrLO"
      },
      "outputs": [],
      "source": [
        "def get_model(model_name):\n",
        "    quant_config = get_quantization_config()\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map='auto',\n",
        "        torch_dtype=torch.float16,\n",
        "        use_auth_token=True,\n",
        "        quantization_config=quant_config,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lJwASe9pS9MS"
      },
      "outputs": [],
      "source": [
        "def get_messages(from_lang, to_lang, code, docstring=False, unit_test=False):\n",
        "    messages = [\n",
        "        {\n",
        "            'role': 'system',\n",
        "            'content': system_message\n",
        "        },\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': get_user_prompt(from_lang, to_lang, code, docstring, unit_test)\n",
        "        }\n",
        "    ]\n",
        "    return messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "CXErPz9FzV1F"
      },
      "outputs": [],
      "source": [
        "def stream_closed_source_model(model_type, model_name, from_lang, to_lang, code, docstring=False, unit_test=False):\n",
        "    messages = get_messages(from_lang, to_lang, code, docstring, unit_test)\n",
        "    accumulated_text = \"\"\n",
        "\n",
        "    try:\n",
        "        if model_type == \"openai\":\n",
        "            response = openai_client.chat.completions.create(\n",
        "                model=model_name,\n",
        "                messages=[\n",
        "                    {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n",
        "                    for m in messages\n",
        "                ],\n",
        "                temperature=0.7,\n",
        "                max_tokens=4096,\n",
        "                stream=True\n",
        "            )\n",
        "\n",
        "            for chunk in response:\n",
        "                if chunk.choices[0].delta.content is not None:\n",
        "                    accumulated_text += chunk.choices[0].delta.content\n",
        "                    yield accumulated_text\n",
        "\n",
        "        elif model_type == \"anthropic\":\n",
        "            with claude_client.messages.stream(\n",
        "                model=model_name,\n",
        "                system=system_message,\n",
        "                messages=[\n",
        "                   messages[1]\n",
        "                ],\n",
        "                max_tokens=4096\n",
        "            ) as stream:\n",
        "                for text in stream.text_stream:\n",
        "                    accumulated_text += text\n",
        "                    yield accumulated_text\n",
        "\n",
        "        elif model_type == \"grok\":\n",
        "            response = grok_client.chat.completions.create(\n",
        "                model=model_name,\n",
        "                messages=[\n",
        "                    {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n",
        "                    for m in messages\n",
        "                ],\n",
        "                temperature=0.7,\n",
        "                max_tokens=4096,\n",
        "                stream=True\n",
        "            )\n",
        "\n",
        "            for chunk in response:\n",
        "                if chunk.choices[0].delta.content is not None:\n",
        "                    accumulated_text += chunk.choices[0].delta.content\n",
        "                    yield accumulated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"API Error: {str(e)}\"\n",
        "        accumulated_text += f\"\\n{error_msg}\"\n",
        "        yield accumulated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "zgSih9_phMuK"
      },
      "outputs": [],
      "source": [
        "def stream_open_source_model(model_name, from_lang, to_lang, code, docstring=False, unit_test=False):\n",
        "    try:\n",
        "        tokenizer = get_tokenizer(model_name)\n",
        "        model = get_model(model_name)\n",
        "\n",
        "        messages = get_messages(from_lang, to_lang, code, docstring, unit_test)\n",
        "        text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "\n",
        "        model_inputs = tokenizer(text, return_tensors='pt').to('cuda')\n",
        "\n",
        "\n",
        "        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "        queue = Queue()\n",
        "\n",
        "\n",
        "        def generate():\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    model.generate(\n",
        "                        **model_inputs,\n",
        "                        max_new_tokens=4096,\n",
        "                        do_sample=True,\n",
        "                        temperature=0.7,\n",
        "                        top_p=0.95,\n",
        "                        streamer=streamer\n",
        "                    )\n",
        "            except Exception as e:\n",
        "                queue.put(f\"Error in generation: {str(e)}\")\n",
        "\n",
        "\n",
        "        thread = threading.Thread(target=generate)\n",
        "        thread.start()\n",
        "\n",
        "\n",
        "        accumulated_text = \"\"\n",
        "\n",
        "\n",
        "        for text_chunk in streamer:\n",
        "            accumulated_text += text_chunk\n",
        "            yield accumulated_text\n",
        "\n",
        "        thread.join()\n",
        "\n",
        "\n",
        "        if not queue.empty():\n",
        "            error_msg = queue.get()\n",
        "            accumulated_text += f\"\\n{error_msg}\"\n",
        "            yield accumulated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        yield f\"Error initializing model: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0v-1FfHI-gVu"
      },
      "outputs": [],
      "source": [
        "def convert_code(model_choice, from_lang, to_lang, code, include_docstring, include_tests):\n",
        "    # Validate input code\n",
        "    if not code or len(code.strip()) == 0:\n",
        "        yield \"Please enter some source code to convert\"\n",
        "        return\n",
        "\n",
        "    if model_choice == \"OpenAI GPT-4o Mini\":\n",
        "        yield from stream_closed_source_model(\"openai\", OPENAI_MODEL, from_lang, to_lang, code, include_docstring, include_tests)\n",
        "    elif model_choice == \"Anthropic Claude 3.7 Sonnet\":\n",
        "        yield from stream_closed_source_model(\"anthropic\", CLAUDE_MODEL, from_lang, to_lang, code, include_docstring, include_tests)\n",
        "    elif model_choice == \"Grok 2.1\":\n",
        "        yield from stream_closed_source_model(\"grok\", GROK_MODEL, from_lang, to_lang, code, include_docstring, include_tests)\n",
        "    elif model_choice == \"Code Lamma 13B\":\n",
        "        yield from stream_open_source_model(CODE_LAMA, from_lang, to_lang, code, include_docstring, include_tests)\n",
        "    else:\n",
        "        yield \"Please select a valid model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ZrvimXYw-kQ9"
      },
      "outputs": [],
      "source": [
        "def create_gradio_interface():\n",
        "    with gr.Blocks(title=\"Code Language Converter\") as app:\n",
        "        gr.Markdown(\"# Code Language Converter\")\n",
        "        gr.Markdown(\"Convert code between different programming languages using AI models\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                model_choice = gr.Dropdown(\n",
        "                    choices=[\n",
        "                        \"OpenAI GPT-4o Mini\",\n",
        "                        \"Anthropic Claude 3.7 Sonnet\",\n",
        "                        \"Grok 2.1\",\n",
        "                        \"Code Lamma 13B\",\n",
        "                    ],\n",
        "                    label=\"Select AI Model\",\n",
        "                    value=\"Anthropic Claude 3.7 Sonnet\"\n",
        "                )\n",
        "\n",
        "                from_lang = gr.Dropdown(\n",
        "                    choices=[\"Python\", \"JavaScript\", \"PHP\", \"C++\", \"Golang\"],\n",
        "                    label=\"Source Language\",\n",
        "                    value=\"Python\"\n",
        "                )\n",
        "\n",
        "                to_lang = gr.Dropdown(\n",
        "                    choices=[\"Python\", \"JavaScript\", \"PHP\", \"Golang\"],\n",
        "                    label=\"Target Language\",\n",
        "                    value=\"JavaScript\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    include_docstring = gr.Checkbox(label=\"Write Docstrings\", value=True)\n",
        "                    include_tests = gr.Checkbox(label=\"Write Unit Tests\", value=False)\n",
        "\n",
        "                convert_button = gr.Button(\"Convert Code\", variant=\"primary\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                source_code = gr.TextArea(\n",
        "                    label=\"Source Code\",\n",
        "                    placeholder=\"Paste your code here...\",\n",
        "                    lines=15\n",
        "                )\n",
        "\n",
        "            with gr.Column():\n",
        "                output = gr.Markdown(\n",
        "                    label=\"Converted Code\",\n",
        "                )\n",
        "\n",
        "        convert_button.click(\n",
        "            fn=convert_code,\n",
        "            inputs=[model_choice, from_lang, to_lang, source_code, include_docstring, include_tests],\n",
        "            outputs=output\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### How to use:\n",
        "        1. Select your AI model of choice\n",
        "        2. Choose the source and target programming languages\n",
        "        3. Optionally select to include docstrings and/or unit tests\n",
        "        4. Paste your code in the Source Code box\n",
        "        5. Click \"Convert Code\" button and watch the response stream in real-time\n",
        "\n",
        "\n",
        "        \"\"\")\n",
        "\n",
        "    return app\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "jJi_z6qf-pal",
        "outputId": "2a785618-33ce-43fe-9627-296fd312e33f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://1348d41ac31c805446.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1348d41ac31c805446.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://1348d41ac31c805446.gradio.live\n"
          ]
        }
      ],
      "source": [
        "# Launch the Gradio interface\n",
        "if __name__ == \"__main__\":\n",
        "    app = create_gradio_interface()\n",
        "    app.launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D01UbzY0-tHd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}